# multi-region-inference-serving-01
Playing around with serving an LLM app across multiple region using GKE
